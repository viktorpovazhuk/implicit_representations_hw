{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import os\n",
    "import gc\n",
    "from importlib import reload\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "from numpy.random import default_rng\n",
    "import trimesh\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mviktor_povazhuk\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/viktor/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if LOCAL:\n",
    "    my_secret = \"3954148eac0eeb54c223e2c9e928de862ea74f68\"\n",
    "else:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    my_secret = user_secrets.get_secret(\"wandb_key\") \n",
    "wandb.login(key=my_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('../input/implicit-repr-xyz/') if not LOCAL else Path('../meshes/')\n",
    "MODELS_DIR = Path('../input/implicit-repr-models/') if not LOCAL else Path('./models/')\n",
    "WORK_DIR = Path('/kaggle/working/') if not LOCAL else Path('./working/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloud(Dataset):\n",
    "    def __init__(self, mesh_path, on_surface_points, keep_aspect_ratio=True):\n",
    "        super().__init__()\n",
    "\n",
    "        print(\"Loading point cloud\")\n",
    "        mesh = trimesh.load_mesh(mesh_path)\n",
    "        print(\"Finished loading point cloud\")\n",
    "\n",
    "        coords = mesh.vertices\n",
    "        self.normals = mesh.vertex_normals\n",
    "\n",
    "        # Reshape point cloud such that it lies in bounding box of (-1, 1) (distorts geometry, but makes for high\n",
    "        # sample efficiency)\n",
    "        coords -= np.mean(coords, axis=0, keepdims=True)\n",
    "        if keep_aspect_ratio:\n",
    "            coord_max = np.amax(coords)\n",
    "            coord_min = np.amin(coords)\n",
    "        else:\n",
    "            coord_max = np.amax(coords, axis=0, keepdims=True)\n",
    "            coord_min = np.amin(coords, axis=0, keepdims=True)\n",
    "\n",
    "        self.coords = (coords - coord_min) / (coord_max - coord_min)\n",
    "        self.coords -= 0.5\n",
    "        self.coords *= 2.\n",
    "\n",
    "        self.on_surface_points = on_surface_points\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.coords.shape[0] // self.on_surface_points\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        point_cloud_size = self.coords.shape[0]\n",
    "\n",
    "        off_surface_samples = self.on_surface_points  # **2\n",
    "        total_samples = self.on_surface_points + off_surface_samples\n",
    "\n",
    "        # Random coords\n",
    "        rand_idcs = np.random.choice(point_cloud_size, size=self.on_surface_points)\n",
    "\n",
    "        on_surface_coords = self.coords[rand_idcs, :]\n",
    "        on_surface_normals = self.normals[rand_idcs, :]\n",
    "\n",
    "        off_surface_coords = np.random.uniform(-1, 1, size=(off_surface_samples, 3))\n",
    "        off_surface_normals = np.ones((off_surface_samples, 3)) * -1\n",
    "\n",
    "        sdf = np.zeros((total_samples, 1))  # on-surface = 0\n",
    "        sdf[self.on_surface_points:, :] = -1  # off-surface = -1\n",
    "\n",
    "        coords = np.concatenate((on_surface_coords, off_surface_coords), axis=0)\n",
    "        normals = np.concatenate((on_surface_normals, off_surface_normals), axis=0)\n",
    "\n",
    "        return {'coords': torch.from_numpy(coords).float()}, {'sdf': torch.from_numpy(sdf).float(),\n",
    "                                                              'normals': torch.from_numpy(normals).float()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parameters = {\n",
    "    'batch_size': 1400,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading point cloud\n",
      "Finished loading point cloud\n"
     ]
    }
   ],
   "source": [
    "sdf_dataset = PointCloud(DATA_DIR / '0.xyz', on_surface_points=data_parameters['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(sdf_dataset, shuffle=True, batch_size=1, pin_memory=True, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sine_init(m):\n",
    "    if hasattr(m, 'weight'):\n",
    "        num_input = m.weight.size(-1)\n",
    "        # See supplement Sec. 1.5 for discussion of factor 30\n",
    "        m.weight.uniform_(-np.sqrt(6 / num_input) / 30, np.sqrt(6 / num_input) / 30)\n",
    "\n",
    "@torch.no_grad()\n",
    "def first_layer_sine_init(m):\n",
    "    if hasattr(m, 'weight'):\n",
    "        num_input = m.weight.size(-1)\n",
    "        # See paper sec. 3.2, final paragraph, and supplement Sec. 1.5 for discussion of factor 30\n",
    "        m.weight.uniform_(-1 / num_input, 1 / num_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sine(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.sin(30 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siren(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        in_features = 3\n",
    "        out_features = 1\n",
    "        hidden_features = 256\n",
    "        num_hidden_layers = 3\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        self.layers.append(nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_features),\n",
    "            Sine()\n",
    "        ))\n",
    "\n",
    "        for i in range(num_hidden_layers):\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Linear(hidden_features, hidden_features),\n",
    "                Sine()\n",
    "            ))\n",
    "        \n",
    "        self.layers.append(nn.Sequential(\n",
    "            nn.Linear(hidden_features, out_features),\n",
    "            Sine()\n",
    "        ))\n",
    "\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "\n",
    "        self.layers.apply(sine_init)\n",
    "        self.layers[0].apply(first_layer_sine_init)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Enables us to compute gradients w.r.t. coordinates\n",
    "        coords_org = input['coords'].clone().detach().requires_grad_(True)\n",
    "        coords = coords_org\n",
    "\n",
    "        output = self.layers(coords)\n",
    "\n",
    "        return {'model_in': coords_org, 'model_out': output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient(y, x, grad_outputs=None):\n",
    "    if grad_outputs is None:\n",
    "        grad_outputs = torch.ones_like(y)\n",
    "    grad = torch.autograd.grad(y, [x], grad_outputs=grad_outputs, create_graph=True)[0]\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SDFLoss(model_output, gt):\n",
    "    gt_sdf = gt['sdf']\n",
    "    gt_normals = gt['normals']\n",
    "\n",
    "    coords = model_output['model_in']\n",
    "    pred_sdf = model_output['model_out']\n",
    "\n",
    "    gradient = calc_gradient(pred_sdf, coords)\n",
    "\n",
    "    # Wherever boundary_values is not equal to zero, we interpret it as a boundary constraint.\n",
    "    sdf_constraint = torch.where(gt_sdf != -1, pred_sdf, torch.zeros_like(pred_sdf))\n",
    "    inter_constraint = torch.where(gt_sdf != -1, torch.zeros_like(pred_sdf), torch.exp(-1e2 * torch.abs(pred_sdf)))\n",
    "    normal_constraint = torch.where(gt_sdf != -1, 1 - F.cosine_similarity(gradient, gt_normals, dim=-1)[..., None],\n",
    "                                    torch.zeros_like(gradient[..., :1]))\n",
    "    grad_constraint = torch.abs(gradient.norm(dim=-1) - 1)\n",
    "\n",
    "    losses = {\n",
    "        'sdf': torch.abs(sdf_constraint).mean() * 3e3,\n",
    "        'inter': inter_constraint.mean() * 1e2,\n",
    "        'normal_constraint': normal_constraint.mean() * 1e2,\n",
    "        'grad_constraint': grad_constraint.mean() * 5e1\n",
    "    }\n",
    "\n",
    "    full_loss = 0\n",
    "    for loss_name, loss in losses.items():\n",
    "        full_loss += loss.mean()\n",
    "\n",
    "    return full_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_parameters = {\n",
    "    'lr': 1e-4,\n",
    "    'epochs': 1,\n",
    "    'device': device,\n",
    "    'device_number': 1,\n",
    "    'continue': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SirenPl(pl.LightningModule):\n",
    "    def __init__(self, model=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = model if model is not None else Siren()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.model(x)\n",
    "        return y\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        if self.trainer.global_step == 0:\n",
    "            if wandb.run is not None:\n",
    "                wandb.define_metric('train_c_loss', summary='mean')\n",
    "\n",
    "                wandb.define_metric('val_c_loss', summary='mean')\n",
    "\n",
    "        model_input, gt = batch\n",
    "\n",
    "        model_output = self(model_input)\n",
    "        loss = SDFLoss(model_output, gt)\n",
    "        \n",
    "        self.log('train_loss', loss)\n",
    "        self.log('global_step', torch.tensor(self.trainer.global_step, dtype=torch.float32))\n",
    "\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    # Called each training\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(lr=algo_parameters['lr'], params=self.model.parameters())\n",
    "        \n",
    "        return optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SirenPl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_run(model):\n",
    "    print('Model train')\n",
    "    trainer = pl.Trainer(fast_dev_run=True, accelerator=algo_parameters['device'].type,\n",
    "                         devices=algo_parameters['device_number'])\n",
    "    trainer.fit(model=model, train_dataloaders=dataloader)\n",
    "\n",
    "def test_data_pipeline():\n",
    "    input, gt = next(iter(sdf_dataset))\n",
    "    print(input['coords'][:5])\n",
    "    print(gt['sdf'][:5])\n",
    "    print(gt['normals'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/viktor/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0128, -0.1823,  0.6272],\n",
      "        [-0.0874, -0.1921,  0.6574],\n",
      "        [ 0.0131,  0.2925,  0.1196],\n",
      "        [-0.0179, -0.1200, -0.7367],\n",
      "        [-0.0243, -0.1350,  0.6689]])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "tensor([[ 2.7577, -1.3920, -1.2207],\n",
      "        [ 3.5160,  2.6087,  1.7178],\n",
      "        [ 2.3228, -2.2470, -1.5867],\n",
      "        [ 5.0516,  0.3837,  1.5555],\n",
      "        [ 2.1252,  0.3616, -0.9825]])\n",
      "Model train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type  | Params\n",
      "--------------------------------\n",
      "0 | model | Siren | 198 K \n",
      "--------------------------------\n",
      "198 K     Trainable params\n",
      "0         Non-trainable params\n",
      "198 K     Total params\n",
      "0.795     Total estimated model params size (MB)\n",
      "/home/viktor/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/viktor/.local/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf4b33853aa4fcfa701fc4e4e94a35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    }
   ],
   "source": [
    "test_data_pipeline()\n",
    "test_model_run(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    ckpt_path = WORK_DIR\n",
    "    ckpt_path = str(ckpt_path)\n",
    "    \n",
    "    wandb_logger = WandbLogger(project='implicit-representation',\n",
    "                               log_model=True,\n",
    "                               entity='viktor_povazhuk',\n",
    "                               tags=['baseline'],\n",
    "                               notes='Test run')\n",
    "    wandb_logger.experiment.config.update(algo_parameters)\n",
    "    \n",
    "    early_stop_callback = EarlyStopping(monitor=\"train_loss\", min_delta=1e-3, patience=5,\n",
    "                                        verbose=True, mode=\"min\", strict=True)\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        save_top_k=2,\n",
    "        monitor=\"global_step\",\n",
    "        mode=\"max\",\n",
    "        dirpath=ckpt_path,\n",
    "        filename=\"ckpt_{epoch}\",\n",
    "        every_n_epochs=10\n",
    "    )\n",
    "    \n",
    "    trainer = pl.Trainer(max_epochs=algo_parameters['epochs'], accelerator=algo_parameters['device'].type,\n",
    "                         devices=algo_parameters['device_number'], logger=wandb_logger,\n",
    "                         callbacks=[early_stop_callback, checkpoint_callback],\n",
    "                         )\n",
    "    if algo_parameters['continue']:\n",
    "        trainer.fit(model=model, train_dataloaders=dataloader, ckpt_path=str(WORK_DIR / '_.ckpt'))\n",
    "    else:\n",
    "        trainer.fit(model=model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viktor/.local/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type  | Params\n",
      "--------------------------------\n",
      "0 | model | Siren | 198 K \n",
      "--------------------------------\n",
      "198 K     Trainable params\n",
      "0         Non-trainable params\n",
      "198 K     Total params\n",
      "0.795     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76138fab5288417cbc5c7ecbf183cfd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric train_loss improved. New best score: 864.946\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), WORK_DIR / 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exalted-spaceship-2</strong> at: <a href='https://wandb.ai/viktor_povazhuk/implicit-representation/runs/ubltxvzy' target=\"_blank\">https://wandb.ai/viktor_povazhuk/implicit-representation/runs/ubltxvzy</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230603_191430-ubltxvzy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Siren().load_state_dict(torch.load(WORK_DIR / 'model.pth'))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# load mesh -> sample points\n",
    "\n",
    "with torch.no_grad():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00159161 -0.49830121 -0.79619932]\n",
      "[-0.26713738 -0.94750222 -0.17571901]\n",
      "0.9999999999999999\n",
      "[5.23322502 5.23322047 5.23322435]\n"
     ]
    }
   ],
   "source": [
    "mesh = trimesh.load_mesh('../meshes/0.obj')\n",
    "\n",
    "normal = mesh.vertex_normals[0]\n",
    "vertice = mesh.vertices[0]\n",
    "\n",
    "print(vertice)\n",
    "print(normal)\n",
    "print(np.linalg.norm(normal))\n",
    "\n",
    "xyz_normal = [-1.397990, -4.958488, -0.919577]\n",
    "xyz_vertex = [-0.001592, -0.498301, -0.796199]\n",
    "print(xyz_normal / normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00159161 -0.49830121 -0.79619932]\n",
      "[-0.26713738 -0.94750222 -0.17571901]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5.23322502, 5.23322047, 5.23322435])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the mesh from the OBJ file\n",
    "mesh = trimesh.load_mesh('../meshes/0.obj')\n",
    "\n",
    "# s = trimesh.Scene([mesh])\n",
    "\n",
    "# s.show()\n",
    "\n",
    "# Calculate the normals at the surface points\n",
    "normals = mesh.vertex_normals\n",
    "vertices = mesh.vertices\n",
    "\n",
    "# # Access the surface points and their corresponding normals\n",
    "# surface_points = mesh.vertices[mesh.faces]\n",
    "# surface_normals = normals[mesh.faces]\n",
    "\n",
    "# # Print the surface points and their normals\n",
    "# for i in range(5):\n",
    "#     print('Surface point:', surface_points[i])\n",
    "#     print('Normal:', surface_normals[i])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
